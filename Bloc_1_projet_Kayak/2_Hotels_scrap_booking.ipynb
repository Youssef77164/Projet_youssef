{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Kayak](https://seekvectorlogo.com/wp-content/uploads/2018/01/kayak-vector-logo.png)\n",
    "\n",
    "# Plan your trip with Kayak \n",
    "\n",
    "## Company's description üìá\n",
    "\n",
    "<a href=\"https://www.kayak.com\" target=\"_blank\">Kayak</a> is a travel search engine that helps user plan their next trip at the best price.\n",
    "\n",
    "The company was founded in 2004 by Steve Hafner & Paul M. English. After a few rounds of fundraising, Kayak was acquired by <a href=\"https://www.bookingholdings.com/\" target=\"_blank\">Booking Holdings</a> which now holds: \n",
    "\n",
    "* <a href=\"https://booking.com/\" target=\"_blank\">Booking.com</a>\n",
    "* <a href=\"https://kayak.com/\" target=\"_blank\">Kayak</a>\n",
    "* <a href=\"https://www.priceline.com/\" target=\"_blank\">Priceline</a>\n",
    "* <a href=\"https://www.agoda.com/\" target=\"_blank\">Agoda</a>\n",
    "* <a href=\"https://Rentalcars.com/\" target=\"_blank\">RentalCars</a>\n",
    "* <a href=\"https://www.opentable.com/\" target=\"_blank\">OpenTable</a>\n",
    "\n",
    "With over \\$300 million revenue a year, Kayak operates in almost all countries and all languages to help their users book travels accros the globe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project üöß\n",
    "\n",
    "The marketing team needs help on a new project. After doing some user research, the team discovered that **70% of their users who are planning a trip would like to have more information about the destination they are going to**. \n",
    "\n",
    "In addition, user research shows that **people tend to be defiant about the information they are reading if they don't know the brand** which produced the content. \n",
    "\n",
    "Therefore, Kayak Marketing Team would like to create an application that will recommend where people should plan their next holidays. The application should be based on real data about:\n",
    "\n",
    "* Weather \n",
    "* Hotels in the area \n",
    "\n",
    "The application should then be able to recommend the best destinations and hotels based on the above variables at any given time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals üéØ\n",
    "\n",
    "As the project has just started, your team doesn't have any data that can be used to create this application. Therefore, your job will be to: \n",
    "\n",
    "* Scrape data from destinations \n",
    "* Get weather data from each destination \n",
    "* Get hotels' info about each destination\n",
    "* Store all the information above in a data lake\n",
    "* Extract, transform and load cleaned data from your datalake to a data warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scope of this project üñºÔ∏è\n",
    "\n",
    "Marketing team wants to focus first on the best cities to travel to in France. According <a href=\"https://one-week-in.com/35-cities-to-visit-in-france/\" target=\"_blank\">One Week In.com</a> here are the top-35 cities to visit in France: \n",
    "\n",
    "```python \n",
    "[\"Mont Saint Michel\",\n",
    "\"St Malo\",\n",
    "\"Bayeux\",\n",
    "\"Le Havre\",\n",
    "\"Rouen\",\n",
    "\"Paris\",\n",
    "\"Amiens\",\n",
    "\"Lille\",\n",
    "\"Strasbourg\",\n",
    "\"Chateau du Haut Koenigsbourg\",\n",
    "\"Colmar\",\n",
    "\"Eguisheim\",\n",
    "\"Besancon\",\n",
    "\"Dijon\",\n",
    "\"Annecy\",\n",
    "\"Grenoble\",\n",
    "\"Lyon\",\n",
    "\"Gorges du Verdon\",\n",
    "\"Bormes les Mimosas\",\n",
    "\"Cassis\",\n",
    "\"Marseille\",\n",
    "\"Aix en Provence\",\n",
    "\"Avignon\",\n",
    "\"Uzes\",\n",
    "\"Nimes\",\n",
    "\"Aigues Mortes\",\n",
    "\"Saintes Maries de la mer\",\n",
    "\"Collioure\",\n",
    "\"Carcassonne\",\n",
    "\"Ariege\",\n",
    "\"Toulouse\",\n",
    "\"Montauban\",\n",
    "\"Biarritz\",\n",
    "\"Bayonne\",\n",
    "\"La Rochelle\"]\n",
    "```\n",
    "\n",
    "Your team should focus **only on the above cities for your project**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers ü¶Æ\n",
    "\n",
    "To help you achieve this project, here are a few tips that should help you\n",
    "\n",
    "### Get weather data with an API \n",
    "\n",
    "*   Use https://nominatim.org/ to get the gps coordinates of all the cities (no subscription required) Documentation : https://nominatim.org/release-docs/develop/api/Search/\n",
    "\n",
    "*   Use https://openweathermap.org/appid (you have to subscribe to get a free apikey) and https://openweathermap.org/api/one-call-api to get some information about the weather for the 35 cities and put it in a DataFrame\n",
    "\n",
    "*   Determine the list of cities where the weather will be the nicest within the next 7 days For example, you can use the values of daily.pop and daily.rain to compute the expected volume of rain within the next 7 days... But it's only an example, actually you can have different opinions on a what a nice weather would be like üòé Maybe the most important criterion for you is the temperature or humidity, so feel free to change the rules !\n",
    "\n",
    "*   Save all the results in a `.csv` file, you will use it later üòâ You can save all the informations that seem important to you ! Don't forget to save the name of the cities, and also to create a column containing a unique identifier (id) of each city (this is important for what's next in the project)\n",
    "\n",
    "*   Use plotly to display the best destinations on a map\n",
    "\n",
    "### Scrape Booking.com \n",
    "\n",
    "Since BookingHoldings doesn't have aggregated databases, it will be much faster to scrape data directly from booking.com \n",
    "\n",
    "You can scrap as many information asyou want, but we suggest that you get at least:\n",
    "\n",
    "*   hotel name,\n",
    "*   Url to its booking.com page,\n",
    "*   Its coordinates: latitude and longitude\n",
    "*   Score given by the website users\n",
    "*   Text description of the hotel\n",
    "\n",
    "\n",
    "### Create your data lake using S3 \n",
    "\n",
    "Once you managed to build your dataset, you should store into S3 as a csv file. \n",
    "\n",
    "### ETL \n",
    "\n",
    "Once you uploaded your data onto S3, it will be better for the next data analysis team to extract clean data directly from a Data Warehouse. Therefore, create a SQL Database using AWS RDS, extract your data from S3 and store it in your newly created DB. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable üì¨\n",
    "\n",
    "To complete this project, your team should deliver:\n",
    "\n",
    "* A `.csv` file in an S3 bucket containing enriched information about weather and hotels for each french city\n",
    "\n",
    "* A SQL Database where we should be able to get the same cleaned data from S3 \n",
    "\n",
    "* Two maps where you should have a Top-5 destinations and a Top-20 hotels in the area. You can use plotly or any other library to do so. It should look something like this: \n",
    "\n",
    "![Map](https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/Kayak_best_destination_project.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - H√¥tels par Villes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Scrapy\n",
      "  Using cached Scrapy-2.5.1-py2.py3-none-any.whl (254 kB)\n",
      "Collecting PyDispatcher>=2.0.5\n",
      "  Downloading PyDispatcher-2.0.5.zip (47 kB)\n",
      "     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47 kB 4.3 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cssselect>=0.9.1\n",
      "  Using cached cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting lxml>=3.5.0\n",
      "  Downloading lxml-4.7.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.9 MB)\n",
      "     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.9 MB 20.5 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: cryptography>=2.0 in /opt/conda/lib/python3.9/site-packages (from Scrapy) (36.0.1)\n",
      "Collecting parsel>=1.5.0\n",
      "  Using cached parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting queuelib>=1.4.2\n",
      "  Using cached queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
      "Collecting h2<4.0,>=3.0\n",
      "  Using cached h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in /opt/conda/lib/python3.9/site-packages (from Scrapy) (22.0.0)\n",
      "Collecting zope.interface>=4.1.3\n",
      "  Downloading zope.interface-5.4.0-cp39-cp39-manylinux2010_x86_64.whl (255 kB)\n",
      "     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 255 kB 63.5 MB/s            \n",
      "\u001b[?25hCollecting itemadapter>=0.1.0\n",
      "  Using cached itemadapter-0.4.0-py3-none-any.whl (10 kB)\n",
      "Collecting w3lib>=1.17.0\n",
      "  Using cached w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n",
      "Collecting service-identity>=16.0.0\n",
      "  Using cached service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting protego>=0.1.15\n",
      "  Using cached Protego-0.1.16.tar.gz (3.2 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting itemloaders>=1.0.1\n",
      "  Using cached itemloaders-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting Twisted[http2]>=17.9.0\n",
      "  Downloading Twisted-22.1.0-py3-none-any.whl (3.1 MB)\n",
      "     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.1 MB 48.5 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.9/site-packages (from cryptography>=2.0->Scrapy) (1.15.0)\n",
      "Collecting hyperframe<6,>=5.2.0\n",
      "  Using cached hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting hpack<4,>=3.0\n",
      "  Using cached hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting jmespath>=0.9.5\n",
      "  Using cached jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: six>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from parsel>=1.5.0->Scrapy) (1.16.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /opt/conda/lib/python3.9/site-packages (from service-identity>=16.0.0->Scrapy) (21.4.0)\n",
      "Collecting pyasn1\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting pyasn1-modules\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting constantly>=15.1\n",
      "  Using cached constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /opt/conda/lib/python3.9/site-packages (from Twisted[http2]>=17.9.0->Scrapy) (4.0.1)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Using cached hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "Collecting Automat>=0.8.0\n",
      "  Using cached Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting incremental>=21.3.0\n",
      "  Using cached incremental-21.3.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting priority<2.0,>=1.1.0\n",
      "  Using cached priority-1.3.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from zope.interface>=4.1.3->Scrapy) (59.8.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=2.0->Scrapy) (2.21)\n",
      "Requirement already satisfied: idna>=2.5 in /opt/conda/lib/python3.9/site-packages (from hyperlink>=17.1.1->Twisted[http2]>=17.9.0->Scrapy) (3.3)\n",
      "Building wheels for collected packages: protego, PyDispatcher\n",
      "  Building wheel for protego (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for protego: filename=Protego-0.1.16-py3-none-any.whl size=7783 sha256=813a80d5a74129dfa9bbe84b8da7eaf8e5bc632c6b8c2cdcc42ffe913ee10e3a\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/9b/e7/dd/5c83b657359b8cc1e116bfab153f22ee891862f6d78d1ddb82\n",
      "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-py3-none-any.whl size=11516 sha256=aad91f7c1fc50bc518209ef310a74a83c33bf8c909c981b9124748f12608194c\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/a5/de/8a/4b52190a95d99c042ec6bd5ad2de3a3c1b5ce71d69f0bbd036\n",
      "Successfully built protego PyDispatcher\n",
      "Installing collected packages: zope.interface, w3lib, pyasn1, lxml, incremental, hyperlink, hyperframe, hpack, cssselect, constantly, Automat, Twisted, pyasn1-modules, priority, parsel, jmespath, itemadapter, h2, service-identity, queuelib, PyDispatcher, protego, itemloaders, Scrapy\n",
      "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 Scrapy-2.5.1 Twisted-22.1.0 constantly-15.1.0 cssselect-1.1.0 h2-3.2.0 hpack-3.0.0 hyperframe-5.2.0 hyperlink-21.0.0 incremental-21.3.0 itemadapter-0.4.0 itemloaders-1.0.4 jmespath-0.10.0 lxml-4.7.1 parsel-1.6.0 priority-1.3.0 protego-0.1.16 pyasn1-0.4.8 pyasn1-modules-0.2.8 queuelib-1.6.2 service-identity-21.1.0 w3lib-1.22.0 zope.interface-5.4.0\n"
     ]
    }
   ],
   "source": [
    "#Suivi des liens de pagination üìÑüìÑüìÑ\n",
    "!pip install Scrapy\n",
    "#L'exemple ci-dessous montre comment utiliser des liens pour it√©rer sur plusieurs pages :\n",
    "import json\n",
    "import requests\n",
    "# Import os => Biblioth√®que utilis√©e pour manipuler facilement les syst√®mes d'exploitation.\n",
    "## Plus d'informations => https://docs.python.org/3/library/os.html\n",
    "import os \n",
    "\n",
    "# Import os => Biblioth√®que utilis√©e pour manipuler facilement les syst√®mes d'exploitation\n",
    "## Plus d'infos => https://docs.python.org/3/library/os.html\n",
    "import logging\n",
    "\n",
    "# Importer scrapy et scrapy.crawler\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Listes des 35 villes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "villes = [\"Le Mont Saint Michel\"]\n",
    "#, \"Uzes\", \"Chateau du Haut Koenigsbourg\", \"Ariege\", \"Aigues Mortes\"\n",
    "#     ,\"St Malo\",\"Bayeux\",\"Le Havre\",\"Rouen\",\"Paris\",\"Amiens\",\"Lille\",\"Strasbourg\",\n",
    "#               \"Colmar\",\"Eguisheim\",\"Besancon\",\"Dijon\",\"Annecy\",\"Grenoble\",\"Lyon\",\n",
    "#               \"Gorges du Verdon\",\"Bormes les Mimosas\",\"Cassis\",\"Marseille\",\"Aix en Provence\",\"Avignon\",\"Nimes\",\n",
    "#               \"Saintes Maries de la mer\",\"Collioure\",\"Carcassonne\",\"Toulouse\",\"Montauban\",\n",
    "#               \"Biarritz\",\"Bayonne\",\"La Rochelle\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Scraping avanc√© avec Scrapy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleSpider(scrapy.Spider):\n",
    "\n",
    "    # Nom de votre araign√©e\n",
    "    name = \"multiplepages\"\n",
    "\n",
    "    # Url √† partir de laquelle d√©marrer votre araign√©e    #h3.sr-hotel__title  #span.sr-hotel__name  #div.sr-hotel__title-wrap\n",
    "    # div.sr-hotel__title-wrap h3.sr-hotel__title a.js-sr-hotel-link.hotel_name_link.url span.sr-hotel__name\n",
    "    startURL = 'https://www.booking.com/searchresults.fr.html?ss='\n",
    "  \n",
    "    start_urls = []\n",
    "    \n",
    "    for ville in villes:\n",
    "        start_urls.append(startURL+ville)\n",
    "    \n",
    "    # Fonction callback qui sera appel√©e lors du d√©marrage de votre spider.\n",
    "    # Elle r√©cup√©rera le texte, l'auteur et les balises de la <div> avec class=\"quote\".\n",
    "    def parse (self, response):\n",
    "        #print(response.body)\n",
    "        text = response.css('h1._30227359d._0db903e42::text').get() #resultat su scrapping\n",
    "        #si le scrapping marche pas, en le relance. \n",
    "        if text == None:\n",
    "            print('none : ' + response.url)\n",
    "            yield response.follow(response.url, callback=self.parse) #pour la relance du scrapping\n",
    "        else:\n",
    "            #scrapping OK\n",
    "            #print('ok : ' +response.url)\n",
    "            #print(response.css('h1._30227359d._0db903e42::text').get()) #sorth1\n",
    "            #ville = str(response.css('h1.sorth1::text').get()).split(':')[0].strip()\n",
    "            url = response.url.split(\"?ss=\")\n",
    "            \n",
    "            ville = url[1].split(\"&\")[0].replace(\"%20\", \" \") #nom de la ville (passer en paramettre dans l'url)\n",
    "                        \n",
    "            nb = int(str(response.css('h1._30227359d._0db903e42::text').get()).split(':')[1].split()[0]) #sorth1 #nombre d'hotels trouv√©es.\n",
    "            \n",
    "            #pour la dernier page en cas de pagination.\n",
    "            offset = 0\n",
    "            if \"offset\" in response.url:\n",
    "                offset = int(response.url.split(\"offset=\")[1]) #recuperer la valeur de l'offset depuis l'url.(offset => pagination )\n",
    "                if offset>nb:\n",
    "                    nb = nb - offset + 25 #avoir le nombre exacte d'hotel restant sur la dernier page.\n",
    "            \n",
    "            i=1 #numero d'hotel en cours de scrapping.\n",
    "            \n",
    "            results = response.css('div._fe1927d9e._0811a1b54._a8a1be610._022ee35ec.b9c27d6646.fb3c4512b4.fc21746a73') #liste des hotels trouv√©.\n",
    "            \n",
    "            for title in results: #sr_item_no_dates\n",
    "                #aller sur la page de l'hotel pour recuperer les coordoner gps\n",
    "                gps = str(requests.get(title.css('div._12369ea61 a::attr(href)').get()).content).split(\"data-atlas-latlng=\\\"\")[1].split(\"data-atlas-bbox=\")[0].split(\"\\\"\")[0]\n",
    "                #print(gps\\)\n",
    "                yield {\n",
    "                    'ville': ville,\n",
    "                    'Hotel': str(title.css('div.fde444d7ef._c445487e2::text').get()).strip(), #sr-hotel__name\n",
    "                    'Url': title.css('div._12369ea61 a::attr(href)').get(),\n",
    "                    'Description': str(title.css('div._4abc4c3d5::text').get()).replace(',','.').replace(\"\\r\\n\", \" \").replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip(), #hotel_desc \n",
    "                    'Note' : str(title.css('div._9c5f726ff.bd528f9ea6::text').get()).strip().replace(',','.'), #bui-review-score__badge\n",
    "                    #'GPS_lat_lng' : gps_page.css('p.showMap2 a::attr(href)').get(), #sr_card_address_line\n",
    "                    #'GPS_lat' : gps_page.css('p.address.address_clean a::attr(data-atlas-latlng)').get().split(',')[0],\n",
    "                    #'GPS_lng' : gps_page.css('p.address.address_clean a::attr(data-atlas-latlng)').get().split(',')[1],\n",
    "                    'GPS_lat' : gps.split(',')[0],\n",
    "                    'GPS_lng' : gps.split(',')[1],\n",
    "                    \n",
    "                }\n",
    "                i+=1 # i = i + 1 : hotel suivant\n",
    "                if i>nb: #pour scraper que le nombre exacte d'hotel par page. parce que booking suggere d'autre hotel pour d'autre ville pour completer la page par 25 hotels\n",
    "                    break\n",
    "            \n",
    "            #pour ne pas chercher d'autre page si le resultat est inferieur a 25. parce que booking suggere d'autre pages pour d'autre ville.\n",
    "            if nb<=25 :\n",
    "                return\n",
    "            \n",
    "            #si on est la, alors surement il y a des pages a scraper\n",
    "            #gestion de pagination: si on est sur le premier scrapping.\n",
    "            next_offset = 0\n",
    "            while nb >= next_offset and \"offset\" not in response.url: \n",
    "                next_offset += 25\n",
    "                next_page = response.url + \"&offset=\"+str(next_offset)\n",
    "                yield response.follow(next_page, callback=self.parse)\n",
    "            \n",
    "            #ancienne code de pagination.\n",
    "            #try:\n",
    "            #    # S√©lectionnez le bouton NEXT et enregistrez-le dans next_page.\n",
    "            #    #next_page = response.xpath('//div[@id=\"b_pageNext\"]/a').attrib[\"href\"]\n",
    "            #    next_page = response.css('a.bui-pagination__link.paging-next').attrib[\"href\"]\n",
    "            #    #print(next_page)\n",
    "            #except KeyError:\n",
    "            #    # Dans la derni√®re page, il n'y aura pas de \"href\" et une KeyError sera lev√©e.\n",
    "            #    logging.info('No next page. Terminating crawling process.')\n",
    "            #else:\n",
    "            #    # Si une page suivante est trouv√©e, ex√©cutez la m√©thode d'analyse une fois de plus.\n",
    "            #    yield response.follow(next_page, callback=self.parse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-23 16:05:18 [scrapy.utils.log] INFO: Scrapy 2.5.1 started (bot: scrapybot)\n",
      "2022-01-23 16:05:18 [scrapy.utils.log] INFO: Versions: lxml 4.7.1.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.7.0, Python 3.8.6 | packaged by conda-forge | (default, Oct  7 2020, 19:08:05) - [GCC 7.5.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1h  22 Sep 2020), cryptography 3.1.1, Platform Linux-5.4.129+-x86_64-with-glibc2.10\n",
      "2022-01-23 16:05:18 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'AUTOTHROTTLE_TARGET_CONCURRENCY': 0.1,\n",
      " 'FEED_EXPORT_ENCODING': 'UTF-8',\n",
      " 'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
      "               '(KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'}\n",
      "2022-01-23 16:05:18 [scrapy.extensions.telnet] INFO: Telnet Password: b800511b921d8b63\n",
      "2022-01-23 16:05:18 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.throttle.AutoThrottle']\n",
      "2022-01-23 16:05:18 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-01-23 16:05:18 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-01-23 16:05:18 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-01-23 16:05:18 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-01-23 16:05:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-01-23 16:05:18 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-01-23 16:05:44 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-01-23 16:05:44 [scrapy.extensions.feedexport] INFO: Stored json feed (13 items) in: src_test_json/Ville_hotel22.json\n",
      "2022-01-23 16:05:44 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 351,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 179437,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 25.690858,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 1, 23, 16, 5, 44, 638394),\n",
      " 'httpcompression/response_bytes': 1072939,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'item_scraped_count': 13,\n",
      " 'log_count/INFO': 11,\n",
      " 'memusage/max': 117272576,\n",
      " 'memusage/startup': 117272576,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2022, 1, 23, 16, 5, 18, 947536)}\n",
      "2022-01-23 16:05:44 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Nom du fichier o√π les r√©sultats seront sauvegard√©s\n",
    "filenameHotel = \"Ville_hotel22\"\n",
    "\n",
    "# Si le fichier existe d√©j√†, supprimez-le avant le crawling (parce que Scrapy va concat√©nera les derniers et nouveaux r√©sultats sinon)\n",
    "if filenameHotel in os.listdir('src_test_json/'):\n",
    "    os.remove('src_test_json/' + filenameHotel + '.json')\n",
    "    os.remove('src_test_json/' + filenameHotel + '.csv')\n",
    "    \n",
    "\n",
    "# D√©clarer un nouveau CrawlerProcess avec quelques param√®tres\n",
    "## USER_AGENT => Simule un navigateur sur un OS\n",
    "## LOG_LEVEL => Niveau minimal de journalisation \n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36',\n",
    "    'LOG_LEVEL': logging.INFO,                                              \n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'AUTOTHROTTLE_TARGET_CONCURRENCY' : 0.1,\n",
    "    'FEED_EXPORT_ENCODING' : 'UTF-8',\n",
    "    'Accept-Language': 'fr-FR, fr;q=0.9',\n",
    "    \"FEEDS\": {\n",
    "        'src_test_json/' + filenameHotel + '.json': {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "\n",
    "# Commencez l'exploration en utilisant l'araign√©e que vous avez d√©finie ci-dessus.\n",
    "process.crawl(MultipleSpider)\n",
    "process.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('src_test_json/' + filenameHotel+'.json','r') as f:\n",
    "    data = json.loads(f.read())\n",
    "\n",
    "df = pd.json_normalize(data)\n",
    "df.to_csv ('src_test_json/' + filenameHotel + '.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - H√¥tels des Villes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "villes = [\"Uzes\",\"Mont Saint Michel\"\n",
    "          , \"Chateau du Haut Koenigsbourg\", \"Ariege\", \"Aigues Mortes\",\"St Malo\",\"Bayeux\",\"Le Havre\",\n",
    "          \"Rouen\",\"Paris\",\"Amiens\", \"Lille\",\"Strasbourg\", \"Colmar\",\"Eguisheim\",\"Besancon\",\"Dijon\",\"Annecy\",\"Grenoble\",\"Lyon\",\n",
    "          \"Gorges du Verdon\",\"Bormes les Mimosas\",\"Cassis\",\"Marseille\",\"Aix en Provence\",\"Avignon\",\"Nimes\",\n",
    "          \"Saintes Maries de la mer\",\"Collioure\",\"Carcassonne\",\"Toulouse\",\"Montauban\",\"Biarritz\",\"Bayonne\",\"La Rochelle\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture du CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('2_Ville_hotel1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ville</th>\n",
       "      <th>hotel</th>\n",
       "      <th>url</th>\n",
       "      <th>description</th>\n",
       "      <th>note</th>\n",
       "      <th>GPS_lat</th>\n",
       "      <th>GPS_lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Le Mont Saint Michel</td>\n",
       "      <td>H√¥tel Vert</td>\n",
       "      <td>https://www.booking.com/hotel/fr/vert.fr.html?...</td>\n",
       "      <td>Situ√© √† 2 km du Mont-Saint-Michel. sur la c√¥te...</td>\n",
       "      <td>8.1</td>\n",
       "      <td>48.614700</td>\n",
       "      <td>-1.509617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Le Mont Saint Michel</td>\n",
       "      <td>Le Relais Saint Michel</td>\n",
       "      <td>https://www.booking.com/hotel/fr/le-relais-sai...</td>\n",
       "      <td>Le Relais Saint Michel vous accueille face √† l...</td>\n",
       "      <td>7.8</td>\n",
       "      <td>48.617587</td>\n",
       "      <td>-1.510396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Le Mont Saint Michel</td>\n",
       "      <td>Mercure Mont Saint Michel</td>\n",
       "      <td>https://www.booking.com/hotel/fr/mont-saint-mi...</td>\n",
       "      <td>Install√© dans des espaces verts √† seulement 2 ...</td>\n",
       "      <td>8.2</td>\n",
       "      <td>48.614247</td>\n",
       "      <td>-1.510545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Le Mont Saint Michel</td>\n",
       "      <td>Le Mouton Blanc</td>\n",
       "      <td>https://www.booking.com/hotel/fr/le-mouton-bla...</td>\n",
       "      <td>Situ√© au pied de l'abbaye. le Mouton Blanc Hot...</td>\n",
       "      <td>7.2</td>\n",
       "      <td>48.636023</td>\n",
       "      <td>-1.509896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Le Mont Saint Michel</td>\n",
       "      <td>Les Terrasses Poulard</td>\n",
       "      <td>https://www.booking.com/hotel/fr/les-terrasses...</td>\n",
       "      <td>Occupant 2 b√¢timents diff√©rents au c≈ìur du Mon...</td>\n",
       "      <td>7.3</td>\n",
       "      <td>48.635349</td>\n",
       "      <td>-1.510379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9423</th>\n",
       "      <td>Grenoble</td>\n",
       "      <td>Grand logement quartier Championnet</td>\n",
       "      <td>https://www.booking.com/hotel/fr/grand-logemen...</td>\n",
       "      <td>H√©bergement g√©r√© par un particulier</td>\n",
       "      <td>8.8</td>\n",
       "      <td>45.187509</td>\n",
       "      <td>5.719823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9424</th>\n",
       "      <td>Grenoble</td>\n",
       "      <td>Montagnes russes</td>\n",
       "      <td>https://www.booking.com/hotel/fr/montagnes-rus...</td>\n",
       "      <td>H√©bergement g√©r√© par un particulier</td>\n",
       "      <td>8.2</td>\n",
       "      <td>45.185315</td>\n",
       "      <td>5.718978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9425</th>\n",
       "      <td>Grenoble</td>\n",
       "      <td>B&amp;B H√¥tel Grenoble Centre Verlaine</td>\n",
       "      <td>https://www.booking.com/hotel/fr/b-amp-b-greno...</td>\n",
       "      <td>Le B&amp;B H√¥tel Grenoble Centre Verlaine propose ...</td>\n",
       "      <td>6.8</td>\n",
       "      <td>45.161910</td>\n",
       "      <td>5.714983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9426</th>\n",
       "      <td>Grenoble</td>\n",
       "      <td>Le Contemporain, Hyper-centre, 8 pers</td>\n",
       "      <td>https://www.booking.com/hotel/fr/le-contempora...</td>\n",
       "      <td>Dot√© d'une connexion Wi-Fi gratuite et offrant...</td>\n",
       "      <td>9.2</td>\n",
       "      <td>45.189887</td>\n",
       "      <td>5.728864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9427</th>\n",
       "      <td>Grenoble</td>\n",
       "      <td>Place de Metz, Hyper-centre Grenoble, Studio 2...</td>\n",
       "      <td>https://www.booking.com/hotel/fr/place-de-metz...</td>\n",
       "      <td>H√©bergement g√©r√© par un particulier</td>\n",
       "      <td>9.7</td>\n",
       "      <td>45.187456</td>\n",
       "      <td>5.730063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9428 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ville                                              hotel  \\\n",
       "0     Le Mont Saint Michel                                         H√¥tel Vert   \n",
       "1     Le Mont Saint Michel                             Le Relais Saint Michel   \n",
       "2     Le Mont Saint Michel                          Mercure Mont Saint Michel   \n",
       "3     Le Mont Saint Michel                                    Le Mouton Blanc   \n",
       "4     Le Mont Saint Michel                              Les Terrasses Poulard   \n",
       "...                    ...                                                ...   \n",
       "9423              Grenoble                Grand logement quartier Championnet   \n",
       "9424              Grenoble                                   Montagnes russes   \n",
       "9425              Grenoble                 B&B H√¥tel Grenoble Centre Verlaine   \n",
       "9426              Grenoble              Le Contemporain, Hyper-centre, 8 pers   \n",
       "9427              Grenoble  Place de Metz, Hyper-centre Grenoble, Studio 2...   \n",
       "\n",
       "                                                    url  \\\n",
       "0     https://www.booking.com/hotel/fr/vert.fr.html?...   \n",
       "1     https://www.booking.com/hotel/fr/le-relais-sai...   \n",
       "2     https://www.booking.com/hotel/fr/mont-saint-mi...   \n",
       "3     https://www.booking.com/hotel/fr/le-mouton-bla...   \n",
       "4     https://www.booking.com/hotel/fr/les-terrasses...   \n",
       "...                                                 ...   \n",
       "9423  https://www.booking.com/hotel/fr/grand-logemen...   \n",
       "9424  https://www.booking.com/hotel/fr/montagnes-rus...   \n",
       "9425  https://www.booking.com/hotel/fr/b-amp-b-greno...   \n",
       "9426  https://www.booking.com/hotel/fr/le-contempora...   \n",
       "9427  https://www.booking.com/hotel/fr/place-de-metz...   \n",
       "\n",
       "                                            description note    GPS_lat  \\\n",
       "0     Situ√© √† 2 km du Mont-Saint-Michel. sur la c√¥te...  8.1  48.614700   \n",
       "1     Le Relais Saint Michel vous accueille face √† l...  7.8  48.617587   \n",
       "2     Install√© dans des espaces verts √† seulement 2 ...  8.2  48.614247   \n",
       "3     Situ√© au pied de l'abbaye. le Mouton Blanc Hot...  7.2  48.636023   \n",
       "4     Occupant 2 b√¢timents diff√©rents au c≈ìur du Mon...  7.3  48.635349   \n",
       "...                                                 ...  ...        ...   \n",
       "9423                H√©bergement g√©r√© par un particulier  8.8  45.187509   \n",
       "9424                H√©bergement g√©r√© par un particulier  8.2  45.185315   \n",
       "9425  Le B&B H√¥tel Grenoble Centre Verlaine propose ...  6.8  45.161910   \n",
       "9426  Dot√© d'une connexion Wi-Fi gratuite et offrant...  9.2  45.189887   \n",
       "9427                H√©bergement g√©r√© par un particulier  9.7  45.187456   \n",
       "\n",
       "       GPS_lng  \n",
       "0    -1.509617  \n",
       "1    -1.510396  \n",
       "2    -1.510545  \n",
       "3    -1.509896  \n",
       "4    -1.510379  \n",
       "...        ...  \n",
       "9423  5.719823  \n",
       "9424  5.718978  \n",
       "9425  5.714983  \n",
       "9426  5.728864  \n",
       "9427  5.730063  \n",
       "\n",
       "[9428 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"note\"]=df[\"note\"].str.replace(\"None\", \"0.0\").astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - R√©partition des meilleurs h√¥tels par villes (note de satisfaction > 9.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.9/site-packages (5.5.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from plotly) (8.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from plotly) (1.16.0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_5.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install plotly\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pio\n",
    "pio.renderers.default=\"iframe_connected\"\n",
    "\n",
    "mask = (df['note'] > 9.5) \n",
    "data_viz = df.loc[mask,:]\n",
    "\n",
    "fig = px.scatter_mapbox(data_viz, lat=\"GPS_lat\", lon=\"GPS_lng\", color=\"note\",  mapbox_style=\"carto-positron\", animation_frame=\"ville\",size=\"note\", size_max = 20,\n",
    "                        color_continuous_scale=px.colors.diverging.Portland,zoom=0 ,hover_name='hotel')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - On concatene les deux CSV 'meteo1' et 'hotels1' ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 20)\n",
      "(9428, 7)\n",
      "(9415, 26)\n"
     ]
    }
   ],
   "source": [
    "meteo1 = pd.read_csv( \"2_Meteo_ville_temp_max_ciel.csv\")\n",
    "hotels1= pd.read_csv(\"2_Ville_hotel1.csv\")\n",
    "print(meteo1.shape)\n",
    "print(hotels1.shape)\n",
    "#print(\"df_meteo : \" + df_meteo['ville'].unique())\n",
    "#print(\"df_hotels : \" + df_hotels['ville'].unique())\n",
    "\n",
    "#Kayak_data_outer = pd.merge(df_meteo, df_hotels, how=\"outer\", on=[\"ville\"])\n",
    "Kayak_data_Meteo_Hotel = pd.merge(meteo1, hotels1, on=[\"ville\"])\n",
    "#(Kayak_displaydata_inner.head())\n",
    "print(Kayak_data_Meteo_Hotel.shape)\n",
    "#display(Kayak_data.head())\n",
    "#print(\"outer : \" + Kayak_data_outer['ville'].unique())\n",
    "#print(\"inner : \" + Kayak_data_inner['ville'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Convertir 'Kayak_data_Meteo_Hotel' en CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export des donn√©es en CSV\n",
    "\n",
    "Kayak_data_Meteo_Hotel.to_csv('3_Meteo_Hotel_Final.csv')\n",
    "Kayak_data_Meteo_Hotel = pd.read_csv('3_Meteo_Hotel_Final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kayak_data_Meteo_Hotel.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
